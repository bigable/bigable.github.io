---
layout: post
title: 《百面机器学习》第九章 前向神经网络 阅读笔记
category: notes
tag: github
---

**02. 深度神经网络中的激活函数**  

---  

**问题1. 写出常用的激活函数及其导数**  

1.1 Sigmoid激活函数  

1.2 Tanh激活函数  

1.3 ReLU激活函数  

**问题2. 为什么Sigmoid和Tanh激活函数会导致梯度消失**  

2.1 Sigmoid激活函数：  

> 将输入z映射到区间(0,1)  
<br>
<br>当z很大时，f(z)趋近于1  
<br>当z很小时，f(z)趋近于0  
<br>
<br>导数 ${f}'(z)=f(z)(1-f(z))$ 在 z 很大或很小时都会趋近于0  
<br>
<br>由此造成梯度消失的现象  

2.2 Tanh激活函数：  

> 将输入z映射到区间(0,1)  
<br>
<br>当z很大时，f(z)趋近于1  
<br>当z很小时，f(z)趋近于-1  
<br>
<br>导数 ${f}'=1-(f(z))^{2}$ 在 z 很大或很小时都会趋近于0  
<br>
<br>同样造成梯度消失  
<br>
<br>实际上，Tanh相当于Sigmoid的平移  

**问题3. ReLU系列的激活函数相对于Sigmoid和Tanh激活函数的优点是什么**  

3.1 优点：  

>1) Sigmoid和Tanh激活函数均需要计算指数，复杂度高，ReLU只需要一个阈值即可得到激活值  
<br>
<br>2) ReLU的非饱和性可以有效地解决梯度消失的问题，提供相对宽的激活边界  
<br>
<br>3) ReLU的单侧抑制提供了网络的稀疏表达能力  

3.2 局限性  

>ReLU在训练过程中会导致神经元死亡的问题  
<br>
<br>这是由于f(z)=max(0,z)导致负梯度在经过该ReLU单元时被置为0，且在之后也不被任何数据激活  
<br>
<br>即流经该神经元的梯度永远为0，不对任何数据产生影响  
<br>
<br>在实际训练中，如果学习率（Learning Rate）设置较大  
<br>
<br>会导致超过一定比例的神经元不可逆死亡，进而参数梯度无法更新，整个训练过程失败  

