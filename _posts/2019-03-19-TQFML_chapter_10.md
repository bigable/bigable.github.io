---
layout: post
title: 《百面机器学习》第十章 循环神经网络 阅读笔记
category: notes
tag: github
---

**02. 循环神经网络的梯度消失问题**  

---  

**问题. 循环神经网络为什么会出现梯度消失或梯度爆炸，有哪些改进方案**  

*1. BPTT（Back Propagation Through Time）*  

>循环神经网络模型的求解可以采用BPTT（基于时间的反向传播）算法实现  
<br>BPTT实际上是反向传播算法的简单变种  
<br>如果将循环神经网络按时间展开成T层的前馈神经网络来理解，就和普通的反向传播算法一样了  

*2. 雅克比矩阵*  

>传统的神经网络梯度可以表示成连乘的形式  
<br>对应的n×n维矩阵，又被称为雅克比矩阵  
	
*3. 梯度爆炸和梯度消失*  

>由于预测误差是沿着神经网络的每一层传播的  

* 梯度爆炸  

>当雅克比矩阵的最大特征值大于1时  
<br>随着离输出越来越远  
<br>每层的梯度大小会呈指数增长  
<br>导致梯度爆炸  

* 梯度消失  

>反之，当雅克比矩阵的最大特征值小于1  
<br>梯度的大小会呈指数缩小  
<br>产生梯度消失  

*4. 无法捕捉长距离的依赖关系*  

>梯度消失导致无法通过加深网络层次来改善神经网络的预测效果  
<br>无论如何加深网络，只有靠近输出的若干层才真正起到学习的作用  
<br>因此循环神经网络模型很难学习到输入序列中的长距离依赖关系  

*5. 改进方案*  

5.1 梯度爆炸→梯度裁剪  

>当梯度的范式大于某个给定值时  
<br>对梯度进行等比收缩  

5.2 梯度消失→改进模型  

* 前馈神经网络→深度残差网络  

>深度残差网络是对前馈神经网络的改进  
<br>通过残差学习的方式缓解梯度消失的现象  
<br>从而使我们能够学习到更深层的网络表示  

* 循环神经网络→门控机制  

>对于循环神经网络来说  
<br>长短时记忆模型及其变种门控循环单元（Gated recurrent unit, GRU）  
<br>通过加入门控机制，很大程度上弥补了梯度消失所带来的损失  

